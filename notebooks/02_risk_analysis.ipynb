{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Credit Portfolio Risk Analysis\n",
    "\n",
    "This notebook performs comprehensive risk analysis including:\n",
    "- Delinquency roll-rate analysis\n",
    "- Expected Loss calculation (PD × LGD × EAD)\n",
    "- Predictive modeling for delinquency\n",
    "- Early warning indicators\n",
    "- Risk trend visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the portfolio data\n",
    "portfolio_df = pd.read_csv('../data/credit_portfolio.csv')\n",
    "print(f\"Loaded portfolio with {len(portfolio_df):,} customers\")\n",
    "\n",
    "# Focus on approved customers for risk analysis\n",
    "approved_df = portfolio_df[portfolio_df['acceptance_decision'] == 'Approved'].copy()\n",
    "print(f\"Approved customers for analysis: {len(approved_df):,}\")\n",
    "\n",
    "# Display basic portfolio metrics\n",
    "print(f\"\\n=== PORTFOLIO OVERVIEW ===\")\n",
    "print(f\"Total portfolio balance: £{approved_df['balance'].sum()/1e6:.1f}M\")\n",
    "print(f\"Average credit limit: £{approved_df['credit_limit'].mean():,.0f}\")\n",
    "print(f\"Average utilization: {approved_df['utilization_rate'].mean():.1f}%\")\n",
    "print(f\"Delinquency rate: {(approved_df['delinquency_status'] > 0).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1. Delinquency Roll-Rate Analysis\n",
    "\n",
    "Roll rates show the probability of accounts moving between delinquency buckets (Current → 30 → 60 → 90+ days past due)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_roll_rates(df):\n",
    "    \"\"\"\n",
    "    Calculate delinquency roll rates - probability of moving between buckets\n",
    "    \"\"\"\n",
    "    # Simulate historical data by creating previous month's status\n",
    "    # This is a simplified version - in reality you'd have time series data\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create synthetic previous month data\n",
    "    df_prev = df.copy()\n",
    "    \n",
    "    # Simulate backwards transitions (some improvement)\n",
    "    for idx in df_prev.index:\n",
    "        current_status = df_prev.loc[idx, 'delinquency_status']\n",
    "        if current_status == 30 and np.random.random() < 0.3:\n",
    "            df_prev.loc[idx, 'prev_delinquency_status'] = 0\n",
    "        elif current_status == 60 and np.random.random() < 0.4:\n",
    "            df_prev.loc[idx, 'prev_delinquency_status'] = np.random.choice([0, 30])\n",
    "        elif current_status == 90 and np.random.random() < 0.2:\n",
    "            df_prev.loc[idx, 'prev_delinquency_status'] = np.random.choice([30, 60])\n",
    "        else:\n",
    "            # Most likely same or worse\n",
    "            if current_status == 0:\n",
    "                df_prev.loc[idx, 'prev_delinquency_status'] = 0\n",
    "            else:\n",
    "                df_prev.loc[idx, 'prev_delinquency_status'] = max(0, current_status - 30)\n",
    "    \n",
    "    # Create roll rate matrix\n",
    "    roll_rate_df = df_prev.groupby(['prev_delinquency_status', 'delinquency_status']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    roll_rate_pct = roll_rate_df.div(roll_rate_df.sum(axis=1), axis=0) * 100\n",
    "    \n",
    "    return roll_rate_pct\n",
    "\n",
    "# Calculate roll rates\n",
    "roll_rates = calculate_roll_rates(approved_df)\n",
    "print(\"=== DELINQUENCY ROLL RATES ===\")\n",
    "print(\"Rows = Previous Month | Columns = Current Month\")\n",
    "print(roll_rates.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize roll rate matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(roll_rates, annot=True, fmt='.1f', cmap='RdYlGn_r', \n",
    "            cbar_kws={'label': 'Roll Rate (%)'}, \n",
    "            xticklabels=['Current', '30 DPD', '60 DPD', '90+ DPD'],\n",
    "            yticklabels=['Current', '30 DPD', '60 DPD', '90+ DPD'])\n",
    "plt.title('Delinquency Roll Rate Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Current Month Delinquency Status')\n",
    "plt.ylabel('Previous Month Delinquency Status')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key roll rate insights\n",
    "print(\"\\n=== KEY ROLL RATE INSIGHTS ===\")\n",
    "current_to_30 = roll_rates.loc[0, 30] if 30 in roll_rates.columns and 0 in roll_rates.index else 0\n",
    "dpd30_to_60 = roll_rates.loc[30, 60] if 30 in roll_rates.index and 60 in roll_rates.columns else 0\n",
    "dpd60_to_90 = roll_rates.loc[60, 90] if 60 in roll_rates.index and 90 in roll_rates.columns else 0\n",
    "\n",
    "print(f\"Current → 30 DPD: {current_to_30:.1f}%\")\n",
    "print(f\"30 DPD → 60 DPD: {dpd30_to_60:.1f}%\")\n",
    "print(f\"60 DPD → 90+ DPD: {dpd60_to_90:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 2. Expected Loss Calculation (PD × LGD × EAD)\n",
    "\n",
    "**Components:**\n",
    "- **PD (Probability of Default)**: Likelihood of default within 12 months\n",
    "- **LGD (Loss Given Default)**: % of exposure lost if default occurs\n",
    "- **EAD (Exposure at Default)**: Amount exposed when default occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pd_lgd_ead(df):\n",
    "    \"\"\"\n",
    "    Calculate PD, LGD, and EAD for each customer segment\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Probability of Default (PD) - based on current risk indicators\n",
    "    # Score-based PD using logistic function\n",
    "    df['pd_score_based'] = 1 / (1 + np.exp((df['application_score'] - 500) / 100))\n",
    "    \n",
    "    # Adjust PD based on current delinquency status\n",
    "    pd_adjustments = {0: 1.0, 30: 3.0, 60: 8.0, 90: 25.0}\n",
    "    df['delinq_multiplier'] = df['delinquency_status'].map(pd_adjustments)\n",
    "    df['pd_12m'] = np.clip(df['pd_score_based'] * df['delinq_multiplier'] / 100, 0.001, 0.5)\n",
    "    \n",
    "    # 2. Loss Given Default (LGD) - industry benchmarks adjusted for portfolio\n",
    "    # Higher utilization typically means higher LGD\n",
    "    base_lgd = 0.45  # 45% average for unsecured credit cards\n",
    "    utilization_impact = df['utilization_rate'] / 100 * 0.3  # Up to 30% impact\n",
    "    df['lgd'] = np.clip(base_lgd + utilization_impact, 0.2, 0.8)\n",
    "    \n",
    "    # 3. Exposure at Default (EAD) - current balance + potential drawdown\n",
    "    # EAD includes current balance + estimated additional drawdown before default\n",
    "    available_credit = df['credit_limit'] - df['balance']\n",
    "    drawdown_rate = 0.5  # Assume 50% of available credit drawn before default\n",
    "    df['ead'] = df['balance'] + (available_credit * drawdown_rate)\n",
    "    \n",
    "    # 4. Expected Loss = PD × LGD × EAD\n",
    "    df['expected_loss'] = df['pd_12m'] * df['lgd'] * df['ead']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Calculate risk metrics\n",
    "risk_df = calculate_pd_lgd_ead(approved_df)\n",
    "\n",
    "# Portfolio-level risk summary\n",
    "total_ead = risk_df['ead'].sum()\n",
    "total_expected_loss = risk_df['expected_loss'].sum()\n",
    "portfolio_loss_rate = total_expected_loss / total_ead * 100\n",
    "weighted_avg_pd = (risk_df['pd_12m'] * risk_df['ead']).sum() / total_ead * 100\n",
    "weighted_avg_lgd = (risk_df['lgd'] * risk_df['ead']).sum() / total_ead * 100\n",
    "\n",
    "print(\"=== PORTFOLIO RISK METRICS ===\")\n",
    "print(f\"Total EAD: £{total_ead/1e6:.1f}M\")\n",
    "print(f\"Total Expected Loss: £{total_expected_loss/1e6:.2f}M\")\n",
    "print(f\"Portfolio Loss Rate: {portfolio_loss_rate:.2f}%\")\n",
    "print(f\"Weighted Average PD: {weighted_avg_pd:.2f}%\")\n",
    "print(f\"Weighted Average LGD: {weighted_avg_lgd:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk segmentation analysis\n",
    "def analyze_risk_segments(df):\n",
    "    \"\"\"\n",
    "    Analyze risk by different customer segments\n",
    "    \"\"\"\n",
    "    segments = {}\n",
    "    \n",
    "    # By Income Band\n",
    "    income_risk = df.groupby('income_band').agg({\n",
    "        'pd_12m': 'mean',\n",
    "        'lgd': 'mean', \n",
    "        'expected_loss': ['sum', 'mean'],\n",
    "        'ead': 'sum',\n",
    "        'customer_id': 'count'\n",
    "    }).round(4)\n",
    "    income_risk.columns = ['Avg_PD_%', 'Avg_LGD_%', 'Total_EL_£', 'Avg_EL_£', 'Total_EAD_£', 'Customer_Count']\n",
    "    income_risk['Loss_Rate_%'] = (income_risk['Total_EL_£'] / income_risk['Total_EAD_£'] * 100).round(2)\n",
    "    \n",
    "    # By Delinquency Status\n",
    "    delinq_risk = df.groupby('delinquency_status').agg({\n",
    "        'pd_12m': 'mean',\n",
    "        'lgd': 'mean',\n",
    "        'expected_loss': ['sum', 'mean'], \n",
    "        'ead': 'sum',\n",
    "        'customer_id': 'count'\n",
    "    }).round(4)\n",
    "    delinq_risk.columns = ['Avg_PD_%', 'Avg_LGD_%', 'Total_EL_£', 'Avg_EL_£', 'Total_EAD_£', 'Customer_Count']\n",
    "    delinq_risk['Loss_Rate_%'] = (delinq_risk['Total_EL_£'] / delinq_risk['Total_EAD_£'] * 100).round(2)\n",
    "    \n",
    "    # By Region\n",
    "    region_risk = df.groupby('region').agg({\n",
    "        'pd_12m': 'mean',\n",
    "        'expected_loss': 'sum',\n",
    "        'ead': 'sum', \n",
    "        'customer_id': 'count'\n",
    "    }).round(4)\n",
    "    region_risk['Loss_Rate_%'] = (region_risk['expected_loss'] / region_risk['ead'] * 100).round(2)\n",
    "    region_risk = region_risk.sort_values('Loss_Rate_%', ascending=False)\n",
    "    \n",
    "    return income_risk, delinq_risk, region_risk\n",
    "\n",
    "income_risk, delinq_risk, region_risk = analyze_risk_segments(risk_df)\n",
    "\n",
    "print(\"\\n=== RISK BY INCOME BAND ===\")\n",
    "print(income_risk)\n",
    "\n",
    "print(\"\\n=== RISK BY DELINQUENCY STATUS ===\")\n",
    "print(delinq_risk)\n",
    "\n",
    "print(\"\\n=== TOP 5 HIGHEST RISK REGIONS ===\")\n",
    "print(region_risk.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize risk distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Portfolio Risk Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# PD Distribution\n",
    "axes[0, 0].hist(risk_df['pd_12m'] * 100, bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "axes[0, 0].set_title('Probability of Default Distribution')\n",
    "axes[0, 0].set_xlabel('12-Month PD (%)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(weighted_avg_pd, color='black', linestyle='--', label=f'Portfolio Avg: {weighted_avg_pd:.2f}%')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# LGD Distribution\n",
    "axes[0, 1].hist(risk_df['lgd'] * 100, bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[0, 1].set_title('Loss Given Default Distribution')\n",
    "axes[0, 1].set_xlabel('LGD (%)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].axvline(weighted_avg_lgd, color='black', linestyle='--', label=f'Portfolio Avg: {weighted_avg_lgd:.1f}%')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Expected Loss by Income Band\n",
    "income_el = income_risk['Total_EL_£'] / 1000  # Convert to thousands\n",
    "axes[1, 0].bar(income_el.index, income_el.values, alpha=0.7, color='purple')\n",
    "axes[1, 0].set_title('Expected Loss by Income Band')\n",
    "axes[1, 0].set_xlabel('Income Band')\n",
    "axes[1, 0].set_ylabel('Total Expected Loss (£000s)')\n",
    "for i, v in enumerate(income_el.values):\n",
    "    axes[1, 0].text(i, v + max(income_el.values) * 0.01, f'£{v:.0f}k', ha='center')\n",
    "\n",
    "# Regional Risk (Top 8)\n",
    "top_regions = region_risk.head(8)\n",
    "axes[1, 1].barh(range(len(top_regions)), top_regions['Loss_Rate_%'], alpha=0.7, color='teal')\n",
    "axes[1, 1].set_title('Top Risk Regions (Loss Rate %)')\n",
    "axes[1, 1].set_xlabel('Loss Rate (%)')\n",
    "axes[1, 1].set_yticks(range(len(top_regions)))\n",
    "axes[1, 1].set_yticklabels(top_regions.index)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 3. Predictive Modeling for Delinquency Risk\n",
    "\n",
    "Build machine learning models to predict future delinquency risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "def prepare_modeling_data(df):\n",
    "    \"\"\"\n",
    "    Prepare features and target for delinquency prediction\n",
    "    \"\"\"\n",
    "    model_df = df.copy()\n",
    "    \n",
    "    # Create binary delinquency target\n",
    "    model_df['is_delinquent'] = (model_df['delinquency_status'] > 0).astype(int)\n",
    "    \n",
    "    # Feature engineering\n",
    "    model_df['limit_to_income_proxy'] = model_df['credit_limit'] / (model_df['application_score'] + 1)\n",
    "    model_df['balance_to_limit'] = model_df['balance'] / (model_df['credit_limit'] + 1)\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le_income = LabelEncoder()\n",
    "    le_repayment = LabelEncoder()\n",
    "    le_region = LabelEncoder()\n",
    "    \n",
    "    model_df['income_band_encoded'] = le_income.fit_transform(model_df['income_band'])\n",
    "    model_df['repayment_history_encoded'] = le_repayment.fit_transform(model_df['repayment_history'])\n",
    "    model_df['region_encoded'] = le_region.fit_transform(model_df['region'])\n",
    "    \n",
    "    # Select features for modeling\n",
    "    feature_cols = [\n",
    "        'application_score', 'credit_limit', 'balance', 'utilization_rate',\n",
    "        'income_band_encoded', 'repayment_history_encoded', 'region_encoded',\n",
    "        'limit_to_income_proxy', 'balance_to_limit'\n",
    "    ]\n",
    "    \n",
    "    X = model_df[feature_cols]\n",
    "    y = model_df['is_delinquent']\n",
    "    \n",
    "    return X, y, feature_cols, model_df\n",
    "\n",
    "X, y, feature_cols, model_df = prepare_modeling_data(risk_df)\n",
    "\n",
    "print(f\"Modeling dataset shape: {X.shape}\")\n",
    "print(f\"Delinquency rate: {y.mean():.3f}\")\n",
    "print(f\"Features: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data and train models\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "model_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'auc': auc,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} AUC: {auc:.3f}\")\n",
    "    print(f\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves Comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for name, results in model_results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, results['probabilities'])\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {results['auc']:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Delinquency Prediction Models')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Select best model\n",
    "best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['auc'])\n",
    "best_model = model_results[best_model_name]['model']\n",
    "print(f\"\\nBest performing model: {best_model_name} (AUC: {model_results[best_model_name]['auc']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(feature_importance)), feature_importance['importance'])\n",
    "    plt.yticks(range(len(feature_importance)), feature_importance['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Feature Importance - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 5 Most Important Features:\")\n",
    "    print(feature_importance.head())\n",
    "\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'coefficient': best_model.coef_[0]\n",
    "    }).sort_values('coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['red' if x < 0 else 'blue' for x in feature_importance['coefficient']]\n",
    "    plt.barh(range(len(feature_importance)), feature_importance['coefficient'], color=colors, alpha=0.7)\n",
    "    plt.yticks(range(len(feature_importance)), feature_importance['feature'])\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title(f'Feature Coefficients - {best_model_name}')\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Feature Coefficients (sorted by absolute value):\")\n",
    "    print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 4. Early Warning Indicators\n",
    "\n",
    "Identify high-risk customers before they become delinquent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate risk scores for all customers\n",
    "risk_scores = best_model.predict_proba(X)[:, 1]\n",
    "model_df['risk_score'] = risk_scores\n",
    "\n",
    "# Define risk segments based on score percentiles\n",
    "risk_thresholds = {\n",
    "    'Low Risk': np.percentile(risk_scores, 75),\n",
    "    'Medium Risk': np.percentile(risk_scores, 90), \n",
    "    'High Risk': np.percentile(risk_scores, 95),\n",
    "    'Very High Risk': 1.0\n",
    "}\n",
    "\n",
    "def assign_risk_segment(score):\n",
    "    if score <= risk_thresholds['Low Risk']:\n",
    "        return 'Low Risk'\n",
    "    elif score <= risk_thresholds['Medium Risk']:\n",
    "        return 'Medium Risk'\n",
    "    elif score <= risk_thresholds['High Risk']:\n",
    "        return 'High Risk'\n",
    "    else:\n",
    "        return 'Very High Risk'\n",
    "\n",
    "model_df['risk_segment'] = model_df['risk_score'].apply(assign_risk_segment)\n",
    "\n",
    "# Early warning analysis\n",
    "early_warning = model_df.groupby('risk_segment').agg({\n",
    "    'customer_id': 'count',\n",
    "    'is_delinquent': ['sum', 'mean'],\n",
    "    'balance': 'sum',\n",
    "    'expected_loss': 'sum',\n",
    "    'utilization_rate': 'mean',\n",
    "    'application_score': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "early_warning.columns = ['Customer_Count', 'Delinquent_Count', 'Delinquency_Rate', \n",
    "                        'Total_Balance_£', 'Total_Expected_Loss_£', 'Avg_Utilization_%', 'Avg_Score']\n",
    "\n",
    "# Calculate concentration metrics\n",
    "early_warning['Portfolio_%'] = (early_warning['Customer_Count'] / len(model_df) * 100).round(1)\n",
    "early_warning['Balance_%'] = (early_warning['Total_Balance_£'] / early_warning['Total_Balance_£'].sum() * 100).round(1)\n",
    "early_warning['Loss_%'] = (early_warning['Total_Expected_Loss_£'] / early_warning['Total_Expected_Loss_£'].sum() * 100).round(1)\n",
    "\n",
    "print(\"=== EARLY WARNING RISK SEGMENTS ===\")\n",
    "print(early_warning)\n",
    "\n",
    "print(f\"\\n=== HIGH RISK ALERTS ===\")\n",
    "high_risk_customers = len(model_df[model_df['risk_segment'].isin(['High Risk', 'Very High Risk'])])\n",
    "high_risk_balance = model_df[model_df['risk_segment'].isin(['High Risk', 'Very High Risk'])]['balance'].sum()\n",
    "\n",
    "print(f\"High Risk Customers: {high_risk_customers:,} ({high_risk_customers/len(model_df)*100:.1f}% of portfolio)\")\n",
    "print(f\"High Risk Balance: £{high_risk_balance/1e6:.1f}M ({high_risk_balance/model_df['balance'].sum()*100:.1f}% of portfolio)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 highest risk customers for immediate attention\n",
    "high_risk_customers_detail = model_df[model_df['risk_segment'] == 'Very High Risk'].nlargest(10, 'risk_score')\n",
    "alert_customers = high_risk_customers_detail[[\n",
    "    'customer_id', 'risk_score', 'application_score', 'credit_limit', 'balance', \n",
    "    'utilization_rate', 'delinquency_status', 'expected_loss'\n",
    "]].copy()\n",
    "\n",
    "print(\"\\n=== TOP 10 CUSTOMERS REQUIRING IMMEDIATE ATTENTION ===\")\n",
    "print(alert_customers.round(3))\n",
    "\n",
    "# Risk score distribution visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(model_df['risk_score'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(risk_thresholds['Medium Risk'], color='yellow', linestyle='--', label='Medium Risk Threshold')\n",
    "plt.axvline(risk_thresholds['High Risk'], color='orange', linestyle='--', label='High Risk Threshold')\n",
    "plt.axvline(risk_thresholds['Very High Risk'], color='red', linestyle='--', label='Very High Risk Threshold')\n",
    "plt.xlabel('Risk Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Risk Score Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "segment_counts = model_df['risk_segment'].value_counts()\n",
    "plt.pie(segment_counts.values, labels=segment_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Portfolio Risk Segmentation')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "risk_vs_delinq = model_df.groupby('risk_segment')['is_delinquent'].mean() * 100\n",
    "colors = ['green', 'yellow', 'orange', 'red']\n",
    "plt.bar(risk_vs_delinq.index, risk_vs_delinq.values, color=colors, alpha=0.7)\n",
    "plt.xlabel('Risk Segment')\n",
    "plt.ylabel('Delinquency Rate (%)')\n",
    "plt.title('Delinquency Rate by Risk Segment')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "loss_by_segment = model_df.groupby('risk_segment')['expected_loss'].sum() / 1000\n",
    "plt.bar(loss_by_segment.index, loss_by_segment.values, color=colors, alpha=0.7)\n",
    "plt.xlabel('Risk Segment')\n",
    "plt.ylabel('Expected Loss (£000s)')\n",
    "plt.title('Expected Loss by Risk Segment')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 5. Risk Monitoring & Trends\n",
    "\n",
    "Simulate portfolio trends and stress scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate portfolio evolution over 12 months\n",
    "def simulate_portfolio_trends(df, months=12):\n",
    "    \"\"\"\n",
    "    Simulate how the portfolio might evolve over time\n",
    "    \"\"\"\n",
    "    trends = []\n",
    "    current_portfolio = df.copy()\n",
    "    \n",
    "    for month in range(months):\n",
    "        # Monthly statistics\n",
    "        month_stats = {\n",
    "            'month': month + 1,\n",
    "            'total_customers': len(current_portfolio),\n",
    "            'total_balance': current_portfolio['balance'].sum(),\n",
    "            'delinquency_rate': (current_portfolio['delinquency_status'] > 0).mean() * 100,\n",
    "            'avg_utilization': current_portfolio['utilization_rate'].mean(),\n",
    "            'expected_loss': current_portfolio['expected_loss'].sum(),\n",
    "            'high_risk_customers': len(current_portfolio[current_portfolio['risk_segment'].isin(['High Risk', 'Very High Risk'])])\n",
    "        }\n",
    "        trends.append(month_stats)\n",
    "        \n",
    "        # Simulate monthly changes (simplified)\n",
    "        # Some customers improve, some deteriorate\n",
    "        np.random.seed(42 + month)\n",
    "        \n",
    "        # Economic cycle impact (simulate recession in months 6-9)\n",
    "        if 6 <= month <= 9:\n",
    "            stress_factor = 1.5  # 50% increase in risk during recession\n",
    "        else:\n",
    "            stress_factor = 1.0\n",
    "        \n",
    "        # Update delinquency status based on risk scores\n",
    "        for idx in current_portfolio.index:\n",
    "            risk_score = current_portfolio.loc[idx, 'risk_score'] * stress_factor\n",
    "            current_status = current_portfolio.loc[idx, 'delinquency_status']\n",
    "            \n",
    "            # Higher risk customers more likely to become delinquent\n",
    "            if risk_score > 0.3 and current_status == 0:\n",
    "                if np.random.random() < risk_score / 5:  # Probabilistic transition\n",
    "                    current_portfolio.loc[idx, 'delinquency_status'] = 30\n",
    "            elif current_status == 30 and np.random.random() < risk_score / 3:\n",
    "                current_portfolio.loc[idx, 'delinquency_status'] = 60\n",
    "            elif current_status == 60 and np.random.random() < risk_score / 2:\n",
    "                current_portfolio.loc[idx, 'delinquency_status'] = 90\n",
    "        \n",
    "        # Recalculate expected loss with new delinquency status\n",
    "        current_portfolio = calculate_pd_lgd_ead(current_portfolio)\n",
    "    \n",
    "    return pd.DataFrame(trends)\n",
    "\n",
    "# Generate trends\n",
    "portfolio_trends = simulate_portfolio_trends(model_df)\n",
    "\n",
    "print(\"=== SIMULATED PORTFOLIO TRENDS (12 MONTHS) ===\")\n",
    "print(portfolio_trends.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize portfolio trends\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Portfolio Risk Trends - 12 Month Simulation', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Delinquency Rate Trend\n",
    "axes[0, 0].plot(portfolio_trends['month'], portfolio_trends['delinquency_rate'], \n",
    "                marker='o', linewidth=2, color='red')\n",
    "axes[0, 0].fill_between([6, 9], 0, portfolio_trends['delinquency_rate'].max() * 1.1, \n",
    "                        alpha=0.2, color='gray', label='Recession Period')\n",
    "axes[0, 0].set_title('Delinquency Rate Trend')\n",
    "axes[0, 0].set_xlabel('Month')\n",
    "axes[0, 0].set_ylabel('Delinquency Rate (%)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Expected Loss Trend\n",
    "axes[0, 1].plot(portfolio_trends['month'], portfolio_trends['expected_loss'] / 1000, \n",
    "                marker='o', linewidth=2, color='orange')\n",
    "axes[0, 1].fill_between([6, 9], 0, portfolio_trends['expected_loss'].max() / 1000 * 1.1, \n",
    "                        alpha=0.2, color='gray', label='Recession Period')\n",
    "axes[0, 1].set_title('Expected Loss Trend')\n",
    "axes[0, 1].set_xlabel('Month')\n",
    "axes[0, 1].set_ylabel('Expected Loss (£000s)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Portfolio Balance Trend\n",
    "axes[1, 0].plot(portfolio_trends['month'], portfolio_trends['total_balance'] / 1e6, \n",
    "                marker='o', linewidth=2, color='blue')\n",
    "axes[1, 0].set_title('Portfolio Balance Trend')\n",
    "axes[1, 0].set_xlabel('Month')\n",
    "axes[1, 0].set_ylabel('Total Balance (£M)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# High Risk Customers Trend\n",
    "axes[1, 1].plot(portfolio_trends['month'], portfolio_trends['high_risk_customers'], \n",
    "                marker='o', linewidth=2, color='purple')\n",
    "axes[1, 1].fill_between([6, 9], 0, portfolio_trends['high_risk_customers'].max() * 1.1, \n",
    "                        alpha=0.2, color='gray', label='Recession Period')\n",
    "axes[1, 1].set_title('High Risk Customers Trend')\n",
    "axes[1, 1].set_xlabel('Month')\n",
    "axes[1, 1].set_ylabel('High Risk Customer Count')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key insights from trends\n",
    "baseline_delinq = portfolio_trends.loc[0, 'delinquency_rate']\n",
    "peak_delinq = portfolio_trends['delinquency_rate'].max()\n",
    "stress_increase = ((peak_delinq - baseline_delinq) / baseline_delinq * 100)\n",
    "\n",
    "baseline_loss = portfolio_trends.loc[0, 'expected_loss']\n",
    "peak_loss = portfolio_trends['expected_loss'].max()\n",
    "loss_increase = ((peak_loss - baseline_loss) / baseline_loss * 100)\n",
    "\n",
    "print(f\"\\n=== STRESS TEST INSIGHTS ===\")\n",
    "print(f\"Baseline Delinquency Rate: {baseline_delinq:.2f}%\")\n",
    "print(f\"Peak Stress Delinquency Rate: {peak_delinq:.2f}%\")\n",
    "print(f\"Stress Impact on Delinquency: +{stress_increase:.1f}%\")\n",
    "print(f\"Stress Impact on Expected Loss: +{loss_increase:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save risk analysis results\n",
    "# Risk-enhanced dataset\n",
    "model_df[[\n",
    "    'customer_id', 'application_score', 'credit_limit', 'balance', 'utilization_rate',\n",
    "    'income_band', 'region', 'delinquency_status', 'repayment_history',\n",
    "    'pd_12m', 'lgd', 'ead', 'expected_loss', 'risk_score', 'risk_segment'\n",
    "]].to_csv('../data/portfolio_with_risk_metrics.csv', index=False)\n",
    "\n",
    "# Risk segment summary\n",
    "early_warning.to_csv('../data/risk_segment_summary.csv')\n",
    "\n",
    "# Portfolio trends\n",
    "portfolio_trends.to_csv('../data/portfolio_trends_simulation.csv', index=False)\n",
    "\n",
    "print(\"=== RISK ANALYSIS COMPLETE ===\")\n",
    "print(f\"✓ Delinquency roll rates calculated\")\n",
    "print(f\"✓ PD/LGD/EAD metrics computed for {len(model_df):,} customers\")\n",
    "print(f\"✓ ML models trained - Best: {best_model_name} (AUC: {model_results[best_model_name]['auc']:.3f})\")\n",
    "print(f\"✓ {high_risk_customers:,} high-risk customers identified\")\n",
    "print(f\"✓ 12-month portfolio trends simulated\")\n",
    "print(f\"✓ Risk analysis results saved to ../data/\")\n",
    "\n",
    "print(f\"\\n=== KEY RISK METRICS ===\")\n",
    "print(f\"Portfolio Loss Rate: {portfolio_loss_rate:.2f}%\")\n",
    "print(f\"Expected Annual Loss: £{total_expected_loss/1e6:.2f}M\")\n",
    "print(f\"Stress Test Peak Loss: £{peak_loss/1e6:.2f}M (+{loss_increase:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}